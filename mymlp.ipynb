{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "mymlp.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JiaSunDeepLearning/AutoEncoder/blob/master/mymlp.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kCBb6P5YpiaC",
        "colab_type": "code",
        "outputId": "91cdc160-0b15-412b-db6c-26ab14b8c767",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 459
        }
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "\n",
        "def compute_gradients(w1, w2, b1, b2, x, y, z):\n",
        "    # 计算梯度\n",
        "    dl_dw1 = w1\n",
        "    dl_dw2 = w2\n",
        "    dl_db1 = b1\n",
        "    dl_db2 = b2\n",
        "    return dl_dw1, dl_dw2, dl_db1, dl_db2\n",
        "\n",
        "\n",
        "def model(w1, w2, b1, b2, x):\n",
        "    # 模型\n",
        "    y = np.dot(w1, x)\n",
        "    z = np.dot(w2, y)\n",
        "    loss = np.linalg.norm(x, keepdims=False)\n",
        "    return y, z, loss\n",
        "\n",
        "\n",
        "def main():\n",
        "    # 配置训练参数\n",
        "    Learn_rate = 0.1\n",
        "    epoch = 10\n",
        "\n",
        "    # 准备数据\n",
        "    x = np.random.randn(64, 1)\n",
        "    label = 1\n",
        "    \n",
        "    # 初始化权重和偏置\n",
        "    w1 = np.random.rand(16, 64)\n",
        "    w2 = np.random.rand(64, 16)\n",
        "    b1 = np.random.rand(16, 1)\n",
        "    b2 = np.random.rand(64, 1)\n",
        "    print(\"model:\\n layer1: Dense(64)\\n layer2: Dense(16)\\n layer3: Dense(64)\\n Dropout(0)\\n\")\n",
        "\n",
        "    i = 1\n",
        "\n",
        "    while(epoch):\n",
        "        # 前向传播\n",
        "        y, z, loss = model(w1, w2, b1, b2, x)\n",
        "\n",
        "        # 梯度计算\n",
        "        dl_dw1, dl_dw2, dl_db1, dl_db2 = compute_gradients(w1, w2, b1, b2, x, y, z)\n",
        "\n",
        "        # 参数调整\n",
        "        w1 = w1 - Learn_rate*dl_dw1\n",
        "        w2 = w2 - Learn_rate*dl_dw2\n",
        "        b1 = b1 - Learn_rate*dl_db1\n",
        "        b2 = b2 - Learn_rate*dl_db2\n",
        "        epoch = epoch - 1\n",
        "        print('epoch: {}, mse: {}\\n'.format(i, loss))\n",
        "        i = i + 1\n",
        "    return 0\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()\n"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "model:\n",
            " layer1: Dense(64)\n",
            " layer2: Dense(16)\n",
            " layer3: Dense(64)\n",
            " Dropout(0)\n",
            "\n",
            "epoch: 1, mse: 7.528226716325359\n",
            "\n",
            "epoch: 2, mse: 7.528226716325359\n",
            "\n",
            "epoch: 3, mse: 7.528226716325359\n",
            "\n",
            "epoch: 4, mse: 7.528226716325359\n",
            "\n",
            "epoch: 5, mse: 7.528226716325359\n",
            "\n",
            "epoch: 6, mse: 7.528226716325359\n",
            "\n",
            "epoch: 7, mse: 7.528226716325359\n",
            "\n",
            "epoch: 8, mse: 7.528226716325359\n",
            "\n",
            "epoch: 9, mse: 7.528226716325359\n",
            "\n",
            "epoch: 10, mse: 7.528226716325359\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}