{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "myautoencoder.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JiaSunDeepLearning/AutoEncoder/blob/master/myautoencoder.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OjBjjp6SoqbE",
        "colab_type": "code",
        "outputId": "ce15b720-be44-45de-b856-2330ba227063",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "try:\n",
        "  # %tensorflow_version only exists in Colab.\n",
        "  %tensorflow_version 2.x\n",
        "except Exception:\n",
        "  pass"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TensorFlow 2.x selected.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dv7sloZBoyUR",
        "colab_type": "code",
        "outputId": "94d0a7b3-d36d-4a35-c61e-0ebb92ad6fbd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from __future__ import absolute_import, division, print_function, unicode_literals\n",
        "\n",
        "# TensorFlow and tf.keras\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "\n",
        "# Helper libraries\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "print(tf.__version__)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2.0.0-beta1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G1OyXyduo6qt",
        "colab_type": "code",
        "outputId": "0f9fb783-7750-471e-ce08-328e7645b27f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "_URL = 'https://storage.googleapis.com/mledu-datasets/cats_and_dogs_filtered.zip'\n",
        "zip_dir = tf.keras.utils.get_file('cats_and_dogs_filterted.zip', origin=_URL, extract=True)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/mledu-datasets/cats_and_dogs_filtered.zip\n",
            "68608000/68606236 [==============================] - 0s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CFMDx6Q8pMvB",
        "colab_type": "code",
        "outputId": "e62fec4c-d035-4894-fc30-21867e6e2a64",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 168
        }
      },
      "source": [
        "import os\n",
        "zip_dir_base = os.path.dirname(zip_dir)\n",
        "!find $zip_dir_base -type d -print\n",
        "print(zip_dir_base)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/root/.keras/datasets\n",
            "/root/.keras/datasets/cats_and_dogs_filtered\n",
            "/root/.keras/datasets/cats_and_dogs_filtered/validation\n",
            "/root/.keras/datasets/cats_and_dogs_filtered/validation/dogs\n",
            "/root/.keras/datasets/cats_and_dogs_filtered/validation/cats\n",
            "/root/.keras/datasets/cats_and_dogs_filtered/train\n",
            "/root/.keras/datasets/cats_and_dogs_filtered/train/dogs\n",
            "/root/.keras/datasets/cats_and_dogs_filtered/train/cats\n",
            "/root/.keras/datasets\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hAOZJAtmpp-p",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from __future__ import absolute_import, division, print_function, unicode_literals\n",
        "import tensorflow as tf\n",
        "import os\n",
        "import time\n",
        "import numpy as np\n",
        "import glob\n",
        "import matplotlib.pyplot as plt\n",
        "# plt.switch_backend('agg')\n",
        "import PIL\n",
        "import imageio\n",
        "from IPython import display\n",
        "from tensorflow.python.keras.preprocessing.image import ImageDataGenerator\n",
        "import pathlib\n",
        "import random\n",
        "import logging\n",
        "\n",
        "logger = tf.get_logger()\n",
        "logger.setLevel(logging.ERROR)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OOPsk4oipzgp",
        "colab_type": "code",
        "outputId": "aa246b87-6233-481c-844c-89369d1f75df",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "base_dir = os.path.join(os.path.dirname(zip_dir), 'cats_and_dogs_filtered')\n",
        "path = os.path.join(base_dir, 'train/')\n",
        "print(path)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/root/.keras/datasets/cats_and_dogs_filtered/train/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F5kOJV8yqsYN",
        "colab_type": "code",
        "outputId": "e48df183-101f-423e-b0cd-2b511206b28c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "BATCH_SIZE = 20\n",
        "IMG_SHAPE = 256\n",
        "\n",
        "\n",
        "class CVAE(tf.keras.Model):\n",
        "    def __init__(self, latent_dim):\n",
        "        super(CVAE, self).__init__()\n",
        "        self.latent_dim = latent_dim\n",
        "        self.inference_net = tf.keras.Sequential(\n",
        "            [\n",
        "                # tf.keras.layers.InputLayer(input_shape=(128, 128, 2)),\n",
        "                tf.keras.layers.Conv2D(\n",
        "                    filters=64, kernel_size=(5, 5), strides=(2, 2), activation='relu', data_format='channels_last',\n",
        "                    padding='valid'),\n",
        "                tf.keras.layers.BatchNormalization(),\n",
        "                tf.keras.layers.Conv2D(\n",
        "                    filters=128, kernel_size=(5, 5), strides=(2, 2), activation='relu', padding='valid'),\n",
        "                tf.keras.layers.BatchNormalization(),\n",
        "                tf.keras.layers.Conv2D(\n",
        "                    filters=256, kernel_size=(5, 5), strides=(2, 2), activation='relu', padding='valid'),\n",
        "                tf.keras.layers.BatchNormalization(),\n",
        "                tf.keras.layers.Conv2D(\n",
        "                    filters=512, kernel_size=(5, 5), strides=(2, 2), activation='relu', padding='valid'),\n",
        "                tf.keras.layers.BatchNormalization(),\n",
        "                tf.keras.layers.Flatten(),\n",
        "                # No activation\n",
        "                tf.keras.layers.Dropout(rate=0.3),\n",
        "                tf.keras.layers.Dense(1024, activation='relu'),\n",
        "\n",
        "                tf.keras.layers.Flatten(),\n",
        "                # No activation\n",
        "                tf.keras.layers.Dropout(rate=0.3),\n",
        "                tf.keras.layers.Dense(2048, activation='relu'),\n",
        "\n",
        "            ]\n",
        "        )\n",
        "\n",
        "        self.generative_net = tf.keras.Sequential(\n",
        "            [\n",
        "                tf.keras.layers.InputLayer(input_shape=(2048,)),\n",
        "                tf.keras.layers.Dropout(rate=0.3),\n",
        "                tf.keras.layers.Dense(units=16 * 16 * 512, activation=tf.nn.relu),\n",
        "                tf.keras.layers.Reshape(target_shape=(16, 16, 512)),\n",
        "                tf.keras.layers.Conv2DTranspose(\n",
        "                    filters=256,\n",
        "                    kernel_size=(5, 5),\n",
        "                    strides=(2, 2),\n",
        "                    padding=\"SAME\",\n",
        "                    activation='relu'),\n",
        "                tf.keras.layers.BatchNormalization(),\n",
        "                tf.keras.layers.Conv2DTranspose(\n",
        "                    filters=128,\n",
        "                    kernel_size=(5, 5),\n",
        "                    strides=(2, 2),\n",
        "                    padding=\"SAME\",\n",
        "                    activation='relu'),\n",
        "                tf.keras.layers.BatchNormalization(),\n",
        "                tf.keras.layers.Conv2DTranspose(\n",
        "                    filters=32,\n",
        "                    kernel_size=(5, 5),\n",
        "                    strides=(2, 2),\n",
        "                    padding=\"SAME\",\n",
        "                    activation='relu'),\n",
        "                tf.keras.layers.BatchNormalization(),\n",
        "                # No activation\n",
        "                tf.keras.layers.Conv2DTranspose(\n",
        "                    filters=3, kernel_size=(5, 5), strides=(1, 1), padding=\"SAME\", activation='relu'),\n",
        "            ]\n",
        "        )\n",
        "\n",
        "    @tf.function\n",
        "    def sample(self, eps=None):\n",
        "        if eps is None:\n",
        "            eps = tf.random.normal(shape=(100, self.latent_dim))\n",
        "        return self.decode(eps, apply_sigmoid=True)\n",
        "\n",
        "    def encode(self, x):\n",
        "        # mean, logvar = tf.split(self.inference_net(x), num_or_size_splits=2, axis=1)\n",
        "        y = self.inference_net(x)\n",
        "        return y\n",
        "\n",
        "    def reparameterize(self, mean, logvar):\n",
        "        eps = tf.random.normal(shape=mean.shape)\n",
        "        return eps * tf.exp(logvar * .5) + mean\n",
        "\n",
        "    def decode(self, z, apply_sigmoid=False):\n",
        "        logits = self.generative_net(z)\n",
        "        if apply_sigmoid:\n",
        "            probs = tf.sigmoid(logits)\n",
        "            return probs\n",
        "\n",
        "        return logits\n",
        "\n",
        "    def sample_forward(self, x):\n",
        "        y = self.inference_net(x)\n",
        "        return y\n",
        "\n",
        "\n",
        "def log_normal_pdf(sample, mean, logvar, raxis=1):\n",
        "    log2pi = tf.math.log(2. * np.pi)\n",
        "    return tf.reduce_sum(\n",
        "        -.5 * ((sample - mean) ** 2. * tf.exp(-logvar) + logvar + log2pi),\n",
        "        axis=raxis)\n",
        "\n",
        "\n",
        "@tf.function\n",
        "def compute_loss(model, x):\n",
        "    z = model.encode(x)\n",
        "    # z = model.reparameterize(mean, logvar)\n",
        "    x_logit = model.decode(z)\n",
        "    mse = tf.keras.losses.MeanSquaredError()\n",
        "\n",
        "    # cross_ent = tf.nn.l2_loss(x_logit-x)\n",
        "    # cross_ent = tf.nn.sigmoid_cross_entropy_with_logits(logits=x_logit, labels=x)\n",
        "    # logpx_z = -tf.reduce_sum(cross_ent, axis=[1, 2, 3])\n",
        "    # logpx_z = -tf.reduce_sum(cross_ent)\n",
        "    # logpz = log_normal_pdf(z, 0., 0.)\n",
        "    # logqz_x = log_normal_pdf(z, mean, logvar)\n",
        "    # return -tf.reduce_mean(logpx_z + logpz - logqz_x)\n",
        "    return mse(x, x_logit)\n",
        "\n",
        "\n",
        "@tf.function\n",
        "def compute_apply_gradients(model, x, optimizer):\n",
        "    with tf.GradientTape() as tape:\n",
        "        loss = compute_loss(model, x)\n",
        "    gradients = tape.gradient(loss, model.trainable_variables)\n",
        "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
        "    return loss\n",
        "\n",
        "\n",
        "def generate_and_save_images(model, epoch, test_input):\n",
        "    predictions = model.sample(test_input)\n",
        "    fig = plt.figure(figsize=(4, 4))\n",
        "\n",
        "    for i in range(predictions.shape[0]):\n",
        "        plt.subplot(4, 4, i + 1)\n",
        "        plt.imshow(predictions[0])\n",
        "        plt.show()\n",
        "        # plt.imshow(predictions[i, :, :, 0], cmap='gray')\n",
        "        plt.axis('off')\n",
        "\n",
        "    # tight_layout minimizes the overlap between 2 sub-plots\n",
        "    plt.savefig('image_at_epoch_{:04d}.png'.format(epoch))\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def valid_model(model, image_input):\n",
        "    t = image_input\n",
        "    z = tf.random.normal(\n",
        "        shape=[1, 512],\n",
        "        mean=125,\n",
        "        stddev=100.0)\n",
        "    predictions = model.sample(z)\n",
        "    plt.imshow(predictions[0])\n",
        "    plt.show()\n",
        "    print(predictions[0])\n",
        "\n",
        "\n",
        "def display_image(epoch_no):\n",
        "    return PIL.Image.open('image_at_epoch_{:04d}.png'.format(epoch_no))\n",
        "\n",
        "\n",
        "def preprocess_image(image):\n",
        "    image = tf.image.decode_jpeg(image, channels=3)\n",
        "    image = tf.image.resize(image, [128, 128])\n",
        "    image /= 255.0  # normalize to [0,1] range\n",
        "\n",
        "    return image\n",
        "\n",
        "\n",
        "def load_and_preprocess_image(path):\n",
        "    image = tf.io.read_file(path)\n",
        "    return preprocess_image(image)\n",
        "\n",
        "\n",
        "def data_load(path):\n",
        "    data_root = pathlib.Path(path)\n",
        "    all_image_paths = list(data_root.glob('*/*'))\n",
        "    all_image_paths = [str(path) for path in all_image_paths]\n",
        "    random.shuffle(all_image_paths)\n",
        "\n",
        "    image_count = len(all_image_paths)\n",
        "\n",
        "    label_names = sorted(item.name for item in data_root.glob('*/') if item.is_dir())\n",
        "\n",
        "    label_to_index = dict((name, index) for index, name in enumerate(label_names))\n",
        "\n",
        "    all_image_labels = [label_to_index[pathlib.Path(path).parent.name]\n",
        "                        for path in all_image_paths]\n",
        "\n",
        "    path_ds = tf.data.Dataset.from_tensor_slices(all_image_paths)\n",
        "\n",
        "    image_ds = path_ds.map(load_and_preprocess_image, num_parallel_calls=tf.data.experimental.AUTOTUNE).batch(5)\n",
        "\n",
        "    label_ds = tf.data.Dataset.from_tensor_slices(tf.cast(all_image_labels, tf.int64))\n",
        "    return image_ds, label_ds, image_count\n",
        "\n",
        "\n",
        "def main():\n",
        "    # path = './cats_and_dogs_filtered/train/'\n",
        "    data_root = pathlib.Path(path)\n",
        "    all_image_paths = list(data_root.glob('*/*'))\n",
        "    all_image_paths = [str(path) for path in all_image_paths]\n",
        "    random.shuffle(all_image_paths)\n",
        "\n",
        "    image_count = len(all_image_paths)\n",
        "\n",
        "    label_names = sorted(item.name for item in data_root.glob('*/') if item.is_dir())\n",
        "\n",
        "    label_to_index = dict((name, index) for index, name in enumerate(label_names))\n",
        "\n",
        "    all_image_labels = [label_to_index[pathlib.Path(path).parent.name]\n",
        "                        for path in all_image_paths]\n",
        "\n",
        "    path_ds = tf.data.Dataset.from_tensor_slices(all_image_paths)\n",
        "\n",
        "    image_ds = path_ds.map(load_and_preprocess_image, num_parallel_calls=tf.data.experimental.AUTOTUNE).batch(10)\n",
        "\n",
        "    label_ds = tf.data.Dataset.from_tensor_slices(tf.cast(all_image_labels, tf.int64))\n",
        "\n",
        "    optimizer = tf.keras.optimizers.Adam(1e-4)\n",
        "\n",
        "    epochs = 500\n",
        "    latent_dim = 512\n",
        "\n",
        "    # evaluate_my_model()\n",
        "\n",
        "    model = CVAE(latent_dim)\n",
        "    for epoch in range(1, epochs + 1):\n",
        "        i = 1\n",
        "        s = 0\n",
        "        start_time = time.time()\n",
        "        for train_x in image_ds:\n",
        "            '''\n",
        "            while i < 1:\n",
        "                plt.imshow(train_x[i])\n",
        "                plt.grid(False)\n",
        "                plt.show()\n",
        "                i = i + 1\n",
        "            valid_model(model, train_x)\n",
        "            '''\n",
        "            loss = compute_apply_gradients(model, train_x, optimizer)\n",
        "            print('Time: {}/{}, Epoch: {}, Batch_mse: {}'.format(i, image_count/BATCH_SIZE, epoch, loss))\n",
        "            i = i + 1\n",
        "            s = s + loss\n",
        "        end_time = time.time()\n",
        "\n",
        "        if epoch % 1 == 0:\n",
        "            '''\n",
        "            loss = tf.keras.metrics.Mean()\n",
        "            # for test_x in val_dataset:\n",
        "            #     loss(compute_loss(model, test_x))\n",
        "            elbo = -loss.result()\n",
        "            display.clear_output(wait=False)\n",
        "            print('Epoch: {}, Test set ELBO: {}, '\n",
        "                  'time elapse for current epoch {}'.format(epoch,\n",
        "                                                            elbo,\n",
        "                                                            end_time - start_time))\n",
        "            '''\n",
        "            print('Epoch:{}, time elapse: {}, epoch_mse: {}'.format(epoch, end_time - start_time, s / i))\n",
        "\n",
        "    model.save_weights('./my_checkpoint/checkpoint')\n",
        "\n",
        "    new_model = CVAE(512)\n",
        "    new_model.load_weights('./my_checkpoint/checkpoint')\n",
        "    '''\n",
        "    print('evaluating')\n",
        "    for image in image_ds:\n",
        "\n",
        "        with tf.GradientTape() as tape:\n",
        "            z = new_model.encode(image)\n",
        "            print(z)\n",
        "            y = new_model.sample(z)\n",
        "            while i < 6:\n",
        "                plt.imshow(y[i])\n",
        "                plt.grid(False)\n",
        "                plt.show()\n",
        "                i = i + 1\n",
        "            break\n",
        "    '''\n",
        "\n",
        "\n",
        "def evaluate_my_model():\n",
        "    path = './cats_and_dogs_filtered/train/'\n",
        "    optimizer = tf.keras.optimizers.Adam(1e-4)\n",
        "    image_ds, label_ds, image_count = data_load(path)\n",
        "\n",
        "    new_model = CVAE(512)\n",
        "    new_model.load_weights('./my_checkpoint/checkpoint')\n",
        "    i = 0\n",
        "    noise = tf.random.normal(\n",
        "        shape=[1, 1024])\n",
        "    img = new_model.decode(noise)\n",
        "    # plt.imshow(img[0]*255)\n",
        "    # plt.show()\n",
        "\n",
        "    for image in image_ds:\n",
        "        z = new_model.encode(image)\n",
        "        print(tf.reduce_max(z), tf.reduce_min(z))\n",
        "        y = new_model.sample(z)\n",
        "        print(tf.reduce_max(y), tf.reduce_min(y))\n",
        "        while i < 5:\n",
        "            plt.imshow(y[i]*255)\n",
        "            plt.grid(False)\n",
        "            plt.show()\n",
        "            i = i + 1\n",
        "            break\n",
        "    return 0\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    train = True\n",
        "    if train:\n",
        "        main()\n",
        "    else:\n",
        "        evaluate_my_model()\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Time: 1/100.0, Epoch: 1, Batch_mse: 0.3047519028186798\n",
            "Time: 2/100.0, Epoch: 1, Batch_mse: 0.21958144009113312\n",
            "Time: 3/100.0, Epoch: 1, Batch_mse: 0.30732351541519165\n",
            "Time: 4/100.0, Epoch: 1, Batch_mse: 0.2819342017173767\n",
            "Time: 5/100.0, Epoch: 1, Batch_mse: 0.24829106032848358\n",
            "Time: 6/100.0, Epoch: 1, Batch_mse: 0.29394763708114624\n",
            "Time: 7/100.0, Epoch: 1, Batch_mse: 0.22329644858837128\n",
            "Time: 8/100.0, Epoch: 1, Batch_mse: 0.23041781783103943\n",
            "Time: 9/100.0, Epoch: 1, Batch_mse: 0.21365800499916077\n",
            "Time: 10/100.0, Epoch: 1, Batch_mse: 0.2807644009590149\n",
            "Time: 11/100.0, Epoch: 1, Batch_mse: 0.227900892496109\n",
            "Time: 12/100.0, Epoch: 1, Batch_mse: 0.1986808031797409\n",
            "Time: 13/100.0, Epoch: 1, Batch_mse: 0.1176140308380127\n",
            "Time: 14/100.0, Epoch: 1, Batch_mse: 0.16996033489704132\n",
            "Time: 15/100.0, Epoch: 1, Batch_mse: 0.1152002364397049\n",
            "Time: 16/100.0, Epoch: 1, Batch_mse: 0.10650797188282013\n",
            "Time: 17/100.0, Epoch: 1, Batch_mse: 0.11754151433706284\n",
            "Time: 18/100.0, Epoch: 1, Batch_mse: 0.10619832575321198\n",
            "Time: 19/100.0, Epoch: 1, Batch_mse: 0.11936721950769424\n",
            "Time: 20/100.0, Epoch: 1, Batch_mse: 0.0876910537481308\n",
            "Time: 21/100.0, Epoch: 1, Batch_mse: 0.09082646667957306\n",
            "Time: 22/100.0, Epoch: 1, Batch_mse: 0.09758935123682022\n",
            "Time: 23/100.0, Epoch: 1, Batch_mse: 0.08883289992809296\n",
            "Time: 24/100.0, Epoch: 1, Batch_mse: 0.09383950382471085\n",
            "Time: 25/100.0, Epoch: 1, Batch_mse: 0.10887511074542999\n",
            "Time: 26/100.0, Epoch: 1, Batch_mse: 0.09759092330932617\n",
            "Time: 27/100.0, Epoch: 1, Batch_mse: 0.08949477970600128\n",
            "Time: 28/100.0, Epoch: 1, Batch_mse: 0.09785265475511551\n",
            "Time: 29/100.0, Epoch: 1, Batch_mse: 0.09285034239292145\n",
            "Time: 30/100.0, Epoch: 1, Batch_mse: 0.10697849839925766\n",
            "Time: 31/100.0, Epoch: 1, Batch_mse: 0.09805859625339508\n",
            "Time: 32/100.0, Epoch: 1, Batch_mse: 0.10293735563755035\n",
            "Time: 33/100.0, Epoch: 1, Batch_mse: 0.08993204683065414\n",
            "Time: 34/100.0, Epoch: 1, Batch_mse: 0.07325390726327896\n",
            "Time: 35/100.0, Epoch: 1, Batch_mse: 0.09830891340970993\n",
            "Time: 36/100.0, Epoch: 1, Batch_mse: 0.08658191561698914\n",
            "Time: 37/100.0, Epoch: 1, Batch_mse: 0.0797472894191742\n",
            "Time: 38/100.0, Epoch: 1, Batch_mse: 0.0912993997335434\n",
            "Time: 39/100.0, Epoch: 1, Batch_mse: 0.0910983607172966\n",
            "Time: 40/100.0, Epoch: 1, Batch_mse: 0.06369990110397339\n",
            "Time: 41/100.0, Epoch: 1, Batch_mse: 0.09357230365276337\n",
            "Time: 42/100.0, Epoch: 1, Batch_mse: 0.09115917980670929\n",
            "Time: 43/100.0, Epoch: 1, Batch_mse: 0.07962308824062347\n",
            "Time: 44/100.0, Epoch: 1, Batch_mse: 0.0847720354795456\n",
            "Time: 45/100.0, Epoch: 1, Batch_mse: 0.08037690818309784\n",
            "Time: 46/100.0, Epoch: 1, Batch_mse: 0.06767240911722183\n",
            "Time: 47/100.0, Epoch: 1, Batch_mse: 0.09178163856267929\n",
            "Time: 48/100.0, Epoch: 1, Batch_mse: 0.07578959316015244\n",
            "Time: 49/100.0, Epoch: 1, Batch_mse: 0.07572360336780548\n",
            "Time: 50/100.0, Epoch: 1, Batch_mse: 0.0794730931520462\n",
            "Time: 51/100.0, Epoch: 1, Batch_mse: 0.07018917798995972\n",
            "Time: 52/100.0, Epoch: 1, Batch_mse: 0.0801144540309906\n",
            "Time: 53/100.0, Epoch: 1, Batch_mse: 0.06639516353607178\n",
            "Time: 54/100.0, Epoch: 1, Batch_mse: 0.06869296729564667\n",
            "Time: 55/100.0, Epoch: 1, Batch_mse: 0.08502040058374405\n",
            "Time: 56/100.0, Epoch: 1, Batch_mse: 0.09017429500818253\n",
            "Time: 57/100.0, Epoch: 1, Batch_mse: 0.06752242892980576\n",
            "Time: 58/100.0, Epoch: 1, Batch_mse: 0.07823540270328522\n",
            "Time: 59/100.0, Epoch: 1, Batch_mse: 0.08524942398071289\n",
            "Time: 60/100.0, Epoch: 1, Batch_mse: 0.0745462030172348\n",
            "Time: 61/100.0, Epoch: 1, Batch_mse: 0.07098294794559479\n",
            "Time: 62/100.0, Epoch: 1, Batch_mse: 0.06271804869174957\n",
            "Time: 63/100.0, Epoch: 1, Batch_mse: 0.07836030423641205\n",
            "Time: 64/100.0, Epoch: 1, Batch_mse: 0.05933661386370659\n",
            "Time: 65/100.0, Epoch: 1, Batch_mse: 0.08492880314588547\n",
            "Time: 66/100.0, Epoch: 1, Batch_mse: 0.066453717648983\n",
            "Time: 67/100.0, Epoch: 1, Batch_mse: 0.07149884849786758\n",
            "Time: 68/100.0, Epoch: 1, Batch_mse: 0.07600124180316925\n",
            "Time: 69/100.0, Epoch: 1, Batch_mse: 0.07317069172859192\n",
            "Time: 70/100.0, Epoch: 1, Batch_mse: 0.05968533083796501\n",
            "Time: 71/100.0, Epoch: 1, Batch_mse: 0.06863631308078766\n",
            "Time: 72/100.0, Epoch: 1, Batch_mse: 0.0631701722741127\n",
            "Time: 73/100.0, Epoch: 1, Batch_mse: 0.07545670121908188\n",
            "Time: 74/100.0, Epoch: 1, Batch_mse: 0.07065887749195099\n",
            "Time: 75/100.0, Epoch: 1, Batch_mse: 0.05969079211354256\n",
            "Time: 76/100.0, Epoch: 1, Batch_mse: 0.07979180663824081\n",
            "Time: 77/100.0, Epoch: 1, Batch_mse: 0.06516272574663162\n",
            "Time: 78/100.0, Epoch: 1, Batch_mse: 0.07626331597566605\n",
            "Time: 79/100.0, Epoch: 1, Batch_mse: 0.06083575636148453\n",
            "Time: 80/100.0, Epoch: 1, Batch_mse: 0.07113923132419586\n",
            "Time: 81/100.0, Epoch: 1, Batch_mse: 0.06577996909618378\n",
            "Time: 82/100.0, Epoch: 1, Batch_mse: 0.06221191957592964\n",
            "Time: 83/100.0, Epoch: 1, Batch_mse: 0.056238412857055664\n",
            "Time: 84/100.0, Epoch: 1, Batch_mse: 0.05876011773943901\n",
            "Time: 85/100.0, Epoch: 1, Batch_mse: 0.057697881013154984\n",
            "Time: 86/100.0, Epoch: 1, Batch_mse: 0.061761099845170975\n",
            "Time: 87/100.0, Epoch: 1, Batch_mse: 0.07315307855606079\n",
            "Time: 88/100.0, Epoch: 1, Batch_mse: 0.0769738107919693\n",
            "Time: 89/100.0, Epoch: 1, Batch_mse: 0.06620343029499054\n",
            "Time: 90/100.0, Epoch: 1, Batch_mse: 0.06426390260457993\n",
            "Time: 91/100.0, Epoch: 1, Batch_mse: 0.0598144456744194\n",
            "Time: 92/100.0, Epoch: 1, Batch_mse: 0.07864488661289215\n",
            "Time: 93/100.0, Epoch: 1, Batch_mse: 0.06712067127227783\n",
            "Time: 94/100.0, Epoch: 1, Batch_mse: 0.06792838871479034\n",
            "Time: 95/100.0, Epoch: 1, Batch_mse: 0.06739456206560135\n",
            "Time: 96/100.0, Epoch: 1, Batch_mse: 0.05443287640810013\n",
            "Time: 97/100.0, Epoch: 1, Batch_mse: 0.06186307594180107\n",
            "Time: 98/100.0, Epoch: 1, Batch_mse: 0.060565460473299026\n",
            "Time: 99/100.0, Epoch: 1, Batch_mse: 0.058514662086963654\n",
            "Time: 100/100.0, Epoch: 1, Batch_mse: 0.05799220874905586\n",
            "Time: 101/100.0, Epoch: 1, Batch_mse: 0.04939407855272293\n",
            "Time: 102/100.0, Epoch: 1, Batch_mse: 0.05649831146001816\n",
            "Time: 103/100.0, Epoch: 1, Batch_mse: 0.0514257475733757\n",
            "Time: 104/100.0, Epoch: 1, Batch_mse: 0.05244617536664009\n",
            "Time: 105/100.0, Epoch: 1, Batch_mse: 0.04935908690094948\n",
            "Time: 106/100.0, Epoch: 1, Batch_mse: 0.06357575953006744\n",
            "Time: 107/100.0, Epoch: 1, Batch_mse: 0.05691172555088997\n",
            "Time: 108/100.0, Epoch: 1, Batch_mse: 0.056055378168821335\n",
            "Time: 109/100.0, Epoch: 1, Batch_mse: 0.05780910328030586\n",
            "Time: 110/100.0, Epoch: 1, Batch_mse: 0.0570882149040699\n",
            "Time: 111/100.0, Epoch: 1, Batch_mse: 0.04421557858586311\n",
            "Time: 112/100.0, Epoch: 1, Batch_mse: 0.04960048943758011\n",
            "Time: 113/100.0, Epoch: 1, Batch_mse: 0.048773422837257385\n",
            "Time: 114/100.0, Epoch: 1, Batch_mse: 0.04982678219676018\n",
            "Time: 115/100.0, Epoch: 1, Batch_mse: 0.05271214246749878\n",
            "Time: 116/100.0, Epoch: 1, Batch_mse: 0.051629386842250824\n",
            "Time: 117/100.0, Epoch: 1, Batch_mse: 0.042342763394117355\n",
            "Time: 118/100.0, Epoch: 1, Batch_mse: 0.057938266545534134\n",
            "Time: 119/100.0, Epoch: 1, Batch_mse: 0.051869623363018036\n",
            "Time: 120/100.0, Epoch: 1, Batch_mse: 0.04983535408973694\n",
            "Time: 121/100.0, Epoch: 1, Batch_mse: 0.03794024512171745\n",
            "Time: 122/100.0, Epoch: 1, Batch_mse: 0.05062035471200943\n",
            "Time: 123/100.0, Epoch: 1, Batch_mse: 0.048141855746507645\n",
            "Time: 124/100.0, Epoch: 1, Batch_mse: 0.05171586945652962\n",
            "Time: 125/100.0, Epoch: 1, Batch_mse: 0.04662368446588516\n",
            "Time: 126/100.0, Epoch: 1, Batch_mse: 0.053498875349760056\n",
            "Time: 127/100.0, Epoch: 1, Batch_mse: 0.03897422179579735\n",
            "Time: 128/100.0, Epoch: 1, Batch_mse: 0.04529677331447601\n",
            "Time: 129/100.0, Epoch: 1, Batch_mse: 0.04154803231358528\n",
            "Time: 130/100.0, Epoch: 1, Batch_mse: 0.048973582684993744\n",
            "Time: 131/100.0, Epoch: 1, Batch_mse: 0.0432945117354393\n",
            "Time: 132/100.0, Epoch: 1, Batch_mse: 0.040383778512477875\n",
            "Time: 133/100.0, Epoch: 1, Batch_mse: 0.04005991294980049\n",
            "Time: 134/100.0, Epoch: 1, Batch_mse: 0.047582969069480896\n",
            "Time: 135/100.0, Epoch: 1, Batch_mse: 0.04822082817554474\n",
            "Time: 136/100.0, Epoch: 1, Batch_mse: 0.03773146867752075\n",
            "Time: 137/100.0, Epoch: 1, Batch_mse: 0.04646114632487297\n",
            "Time: 138/100.0, Epoch: 1, Batch_mse: 0.040950141847133636\n",
            "Time: 139/100.0, Epoch: 1, Batch_mse: 0.04516557231545448\n",
            "Time: 140/100.0, Epoch: 1, Batch_mse: 0.04227997735142708\n",
            "Time: 141/100.0, Epoch: 1, Batch_mse: 0.03877076506614685\n",
            "Time: 142/100.0, Epoch: 1, Batch_mse: 0.03488823026418686\n",
            "Time: 143/100.0, Epoch: 1, Batch_mse: 0.040752097964286804\n",
            "Time: 144/100.0, Epoch: 1, Batch_mse: 0.04538001865148544\n",
            "Time: 145/100.0, Epoch: 1, Batch_mse: 0.033066850155591965\n",
            "Time: 146/100.0, Epoch: 1, Batch_mse: 0.03416035324335098\n",
            "Time: 147/100.0, Epoch: 1, Batch_mse: 0.03921567648649216\n",
            "Time: 148/100.0, Epoch: 1, Batch_mse: 0.03885122016072273\n",
            "Time: 149/100.0, Epoch: 1, Batch_mse: 0.03631941229104996\n",
            "Time: 150/100.0, Epoch: 1, Batch_mse: 0.03984611853957176\n",
            "Time: 151/100.0, Epoch: 1, Batch_mse: 0.04593484848737717\n",
            "Time: 152/100.0, Epoch: 1, Batch_mse: 0.038320355117321014\n",
            "Time: 153/100.0, Epoch: 1, Batch_mse: 0.04776757210493088\n",
            "Time: 154/100.0, Epoch: 1, Batch_mse: 0.03897779807448387\n",
            "Time: 155/100.0, Epoch: 1, Batch_mse: 0.040500789880752563\n",
            "Time: 156/100.0, Epoch: 1, Batch_mse: 0.032531239092350006\n",
            "Time: 157/100.0, Epoch: 1, Batch_mse: 0.035289011895656586\n",
            "Time: 158/100.0, Epoch: 1, Batch_mse: 0.05222247913479805\n",
            "Time: 159/100.0, Epoch: 1, Batch_mse: 0.03968158736824989\n",
            "Time: 160/100.0, Epoch: 1, Batch_mse: 0.040345072746276855\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}